{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "data_train=datasets.MNIST('./data',transform=transform,train=True)\n",
    "data_test=datasets.MNIST('./data',transform=transform,train=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "data_loader_train=DataLoader(dataset=data_train,batch_size=64,shuffle=True)\n",
    "data_loader_test=DataLoader(dataset=data_test,batch_size=64,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(1,32,3,1,1)\n",
    "        self.conv2=nn.Conv2d(32,64,3,1,1)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.fc1=nn.Linear(64*7*7,512)\n",
    "        self.fc2=nn.Linear(512,256)\n",
    "        self.fc3=nn.Linear(256,10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,64*7*7)\n",
    "        x=self.dropout(F.relu(self.fc1(x)))\n",
    "        x=self.dropout(F.relu(self.fc2(x)))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "net=CNN()\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  100] loss :1.839\n",
      "[1,  200] loss :0.762\n",
      "[1,  300] loss :0.511\n",
      "[1,  400] loss :0.373\n",
      "[1,  500] loss :0.311\n",
      "[1,  600] loss :0.275\n",
      "[1,  700] loss :0.223\n",
      "[1,  800] loss :0.217\n",
      "[1,  900] loss :0.192\n",
      "[2,  100] loss :0.177\n",
      "[2,  200] loss :0.159\n",
      "[2,  300] loss :0.160\n",
      "[2,  400] loss :0.134\n",
      "[2,  500] loss :0.129\n",
      "[2,  600] loss :0.121\n",
      "[2,  700] loss :0.118\n",
      "[2,  800] loss :0.112\n",
      "[2,  900] loss :0.124\n",
      "[3,  100] loss :0.105\n",
      "[3,  200] loss :0.102\n",
      "[3,  300] loss :0.100\n",
      "[3,  400] loss :0.094\n",
      "[3,  500] loss :0.095\n",
      "[3,  600] loss :0.077\n",
      "[3,  700] loss :0.090\n",
      "[3,  800] loss :0.085\n",
      "[3,  900] loss :0.084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss = []\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    for i,data in enumerate(data_loader_train,0):#0是下標起始位置預設為0\n",
    "        # data 的格式[[inputs, labels]]       \n",
    "#         inputs,labels = data\n",
    "        inputs,labels = data[0].to(device), data[1].to(device)\n",
    "        #初始為0，清除上個batch的梯度訊息\n",
    "        optimizer.zero_grad()         \n",
    "        #前向+後向+優化\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "        running_loss += loss.item()\n",
    "        if i%100 == 99:\n",
    "            PATH = './mnist_net.pth'\n",
    "            torch.save(net.state_dict(), PATH)#保存訓練結果\n",
    "\n",
    "            print('[%d,%5d] loss :%.3f' %\n",
    "                 (epoch+1,i+1,running_loss/100))\n",
    "            running_loss = 0.0\n",
    "        train_loss.append(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the  test images:  97.62 %\n"
     ]
    }
   ],
   "source": [
    "test_net=CNN()#利用test data測試訓練成果\n",
    "PATH = './mnist_net.pth'\n",
    "test_net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "total=0\n",
    "correct=0\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(data_loader_test,0):\n",
    "        input,label=data\n",
    "        test_out=test_net(input)\n",
    "        _,predicted=torch.max(test_out.data,dim=1)\n",
    "        total+=label.size(0)\n",
    "        correct+=(predicted==label).sum().item()\n",
    "print('Accuracy of the network on the  test images: ',float(100*correct/total),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "show_test=datasets.MNIST('./data',train=False)\n",
    "#test data測試訓練成果--2\n",
    "net=CNN()\n",
    "PATH = './mnist_net.pth'\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "fig=plt.figure(figsize=(2,2))\n",
    "x=random.randint(0,9999)\n",
    "transform=torchvision.transforms.ToTensor()\n",
    "image=transform(show_test[x][0])\n",
    "output=net(image)\n",
    "_,predicted=torch.max(output.data,dim=1)\n",
    "\n",
    "plt.title(\"I guess it is {} (answer={})\".format(predicted.item(),show_test[x][1]))\n",
    "plt.imshow(show_test[x][0])\n",
    "plt.show()\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2#手繪圖測試——1(不要亂用，用來改圖片格式)\n",
    "img_path=\"./drawToTest/1.png\"\n",
    "image=cv2.imread(img_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray_invert=cv2.bitwise_not(gray)\n",
    "print(gray_invert.shape)\n",
    "cv2.imwrite(img_path,gray_invert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor([0])\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([1])\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([2])\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([3])\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([1])\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([5])\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([6])\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([7])\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([3])\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([7])\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image#手繪圖測試——2\n",
    "net=CNN()\n",
    "PATH = './mnist_net.pth'\n",
    "\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "with torch.no_grad():\n",
    "    for x in range(10):\n",
    "        img_path=\"./drawToTest/{}.png\".format(x)\n",
    "        image=PIL.Image.open(img_path)\n",
    "        transform=torchvision.transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        image=transform(image)\n",
    "        print(image.shape)\n",
    "        output=net(image)\n",
    "        _,predicted=torch.max(output.data,dim=1)\n",
    "        print(predicted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
